{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'dslab-group_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4777</td><td>application_1587988164357_1412</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1412/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1412_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4793</td><td>application_1587988164357_1428</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1428/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1428_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4794</td><td>application_1587988164357_1429</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1429/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1429_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4795</td><td>application_1587988164357_1430</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1430/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1430_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4797</td><td>application_1587988164357_1432</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1432/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1432_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4798</td><td>application_1587988164357_1433</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1433/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1433_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4800</td><td>application_1587988164357_1435</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1435/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1435_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4801</td><td>application_1587988164357_1436</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1436/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1436_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4802</td><td>application_1587988164357_1437</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1437/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1437_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4804</td><td>application_1587988164357_1439</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1439/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1439_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4805</td><td>application_1587988164357_1440</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1440/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1440_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4806</td><td>application_1587988164357_1442</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1442/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1442_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4808</td><td>application_1587988164357_1443</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1443/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1443_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4809</td><td>application_1587988164357_1444</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1444/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1444_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4810</td><td>application_1587988164357_1445</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1445/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1445_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4811</td><td>application_1587988164357_1446</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1446/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1446_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>4812</td><td>application_1587988164357_1447</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1447/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1447_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"dslab-group_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4813</td><td>application_1587988164357_1448</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1587988164357_1448/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1587988164357_1448_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from geopy.distance import distance as geo_distance\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading data, these are snapshots of the all available data\n",
    "# Calendar and trips are useful to filter the other dataframe according to the day\n",
    "stop_times = spark.read.format('orc').load('/data/sbb/timetables/orc/stop_times/000000_0')\n",
    "stops = spark.read.format('orc').load('/data/sbb/timetables/orc/stops/000000_0')\n",
    "trips = spark.read.format('orc').load('/data/sbb/timetables/orc/trips/000000_0')\n",
    "calendar = spark.read.format('orc').load('/data/sbb/timetables/orc/calendar/000000_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+-----------+-------------+-----------+-------------+\n",
      "|             trip_id|arrival_time|departure_time|    stop_id|stop_sequence|pickup_type|drop_off_type|\n",
      "+--------------------+------------+--------------+-----------+-------------+-----------+-------------+\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:20:00|      04:20:00|8500010:0:3|            1|          0|            0|\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:24:00|      04:24:00|8500020:0:3|            2|          0|            0|\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:28:00|      04:28:00|8500021:0:5|            3|          0|            0|\n",
      "+--------------------+------------+--------------+-----------+-------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------------+----------------+----------------+-------------+--------------+\n",
      "|stop_id|   stop_name|        stop_lat|        stop_lon|location_type|parent_station|\n",
      "+-------+------------+----------------+----------------+-------------+--------------+\n",
      "|1322000|    Altoggio|46.1672513851495|  8.345807131427|             |              |\n",
      "|1322001|Antronapiana| 46.060121674738|8.11361957990831|             |              |\n",
      "|1322002|      Anzola|45.9898698225697|8.34571729989858|             |              |\n",
      "+-------+------------+----------------+----------------+-------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "|   route_id|service_id|             trip_id|     trip_headsign|trip_short_name|direction_id|\n",
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "|1-1-C-j19-1|  TA+b0001|5.TA.1-1-C-j19-1.3.R|Zofingen, Altachen|            108|           1|\n",
      "|1-1-C-j19-1|  TA+b0001|7.TA.1-1-C-j19-1.3.R|Zofingen, Altachen|            112|           1|\n",
      "|1-1-C-j19-1|  TA+b0001|9.TA.1-1-C-j19-1.3.R|Zofingen, Altachen|            116|           1|\n",
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+------+-------+---------+--------+------+--------+------+\n",
      "|service_id|monday|tuesday|wednesday|thursday|friday|saturday|sunday|\n",
      "+----------+------+-------+---------+--------+------+--------+------+\n",
      "|  TA+b0nx9|  true|   true|     true|    true|  true|   false| false|\n",
      "|  TA+b03bf|  true|   true|     true|    true|  true|   false| false|\n",
      "|  TA+b0008|  true|   true|     true|    true|  true|   false| false|\n",
      "+----------+------+-------+---------+--------+------+--------+------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "stop_times.show(3)\n",
    "stops.show(3)\n",
    "trips.show(3)\n",
    "calendar.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+------------+-------------+-----------+-------------+\n",
      "|             trip_id|arrival_time|departure_time|     stop_id|stop_sequence|pickup_type|drop_off_type|\n",
      "+--------------------+------------+--------------+------------+-------------+-----------+-------------+\n",
      "|1.TA.11-12-B-j19-...|    08:30:00|      08:30:00|     8582915|            1|          0|            0|\n",
      "|1.TA.11-12-B-j19-...|    08:31:00|      08:31:00|     8582916|            2|          0|            0|\n",
      "|1.TA.11-12-B-j19-...|    08:33:00|      08:33:00|     8582917|            3|          0|            0|\n",
      "|1.TA.11-12-B-j19-...|    08:35:00|      08:35:00|     8574680|            4|          0|            0|\n",
      "|1.TA.12-1-A-j19-1...|    01:30:00|      01:30:00|8505000:0:11|            1|          0|            0|\n",
      "+--------------------+------------+--------------+------------+-------------+-----------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "days_dict = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday'}\n",
    "\n",
    "def day_trips(*day_ids):\n",
    "    days = [days_dict[day_id] for day_id in day_ids]\n",
    "    where_clause = \" and \".join(days)\n",
    "\n",
    "    day_services = calendar.where(where_clause).select('service_id')\n",
    "    return day_services.join(trips, on='service_id').select('trip_id')\n",
    "\n",
    "# Example for stop_times filtered on wednesday\n",
    "stop_times_wed = day_trips(2).join(stop_times, on='trip_id')\n",
    "stop_times_wed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Can't run, the count makes it timeout. I asked Tao why\n",
    "#print('Full stop times have', stop_times.count(), 'entries, filtered has', stop_times_wed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of stations around Zurich:', 1880)\n",
      "+-----------+------------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "|    stop_id|   zurich_distance|           stop_name|        stop_lat|        stop_lon|location_type|parent_station|\n",
      "+-----------+------------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "|    8500926|11.510766966884365|Oetwil a.d.L., Sc...|47.4236270123012| 8.4031825286317|             |              |\n",
      "|    8502186|10.798985488832079|Dietikon Stoffelbach|47.3934058321612|8.39894248049007|             |      8502186P|\n",
      "|8502186:0:1|10.800041577194426|Dietikon Stoffelbach|47.3934666445388|8.39894248049007|             |      8502186P|\n",
      "|8502186:0:2|10.801101793198619|Dietikon Stoffelbach|47.3935274568464|8.39894248049007|             |      8502186P|\n",
      "|   8502186P|10.798985488832079|Dietikon Stoffelbach|47.3934058321612|8.39894248049007|            1|              |\n",
      "|    8502187|12.409213630136994|Rudolfstetten Hof...|47.3646945560768|8.37709545277724|             |      8502187P|\n",
      "|8502187:0:1|12.408391334825147|Rudolfstetten Hof...|47.3647554015789|8.37709545277724|             |      8502187P|\n",
      "|8502187:0:2|12.407572674086833|Rudolfstetten Hof...|47.3648162470108|8.37709545277724|             |      8502187P|\n",
      "|   8502187P|12.409213630136994|Rudolfstetten Hof...|47.3646945560768|8.37709545277724|            1|              |\n",
      "|    8502188|14.227431292063317|   Zufikon Hammergut|47.3558347019549|8.35472740219955|             |      8502188P|\n",
      "|8502188:0:1|14.226243718914208|   Zufikon Hammergut|47.3558955576756|8.35472740219955|             |      8502188P|\n",
      "|8502188:0:2| 14.22505926595959|   Zufikon Hammergut|47.3559564133261|8.35472740219955|             |      8502188P|\n",
      "|   8502188P|14.227431292063317|   Zufikon Hammergut|47.3558347019549|8.35472740219955|            1|              |\n",
      "|    8502208|13.797193968894737|     Horgen Oberdorf|47.2587475534877|8.58979854578067|             |      8502208P|\n",
      "|8502208:0:2|13.777624555384257|     Horgen Oberdorf|47.2589304560815|8.58979854578067|             |      8502208P|\n",
      "|8502208:0:3|13.790670577681448|     Horgen Oberdorf|47.2588085210892|8.58979854578067|             |      8502208P|\n",
      "|8502208:0:4|13.784147439735152|     Horgen Oberdorf|47.2588694886204|8.58979854578067|             |      8502208P|\n",
      "|   8502208P|13.797193968894737|     Horgen Oberdorf|47.2587475534877|8.58979854578067|            1|              |\n",
      "|    8502209|11.628857061881249|     Oberrieden Dorf|47.2767238569466|  8.577635356832|             |      8502209P|\n",
      "|8502209:0:1| 11.61571253543463|     Oberrieden Dorf|47.2768457506749|  8.577635356832|             |      8502209P|\n",
      "+-----------+------------------+--------------------+----------------+----------------+-------------+--------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "zurich_pos = stops.where(column('stop_id')=='8503000').select('stop_lat', 'stop_lon').collect()\n",
    "zurich_pos = (zurich_pos[0][0], zurich_pos[0][1])\n",
    "\n",
    "def zurich_distance(x, y):\n",
    "    return geo_distance(zurich_pos, (x,y)).km\n",
    "\n",
    "stops_distance = stops.rdd.map(lambda x: (x['stop_id'], zurich_distance(x['stop_lat'], x['stop_lon'])))\n",
    "stops_distance = spark.createDataFrame(stops_distance.map(lambda r: Row(stop_id=r[0], zurich_distance=r[1])))\n",
    "stops_distance = stops_distance.filter(column('zurich_distance') <= 15)\n",
    "stops_zurich = stops_distance.join(stops, on='stop_id')\n",
    "print('Number of stations around Zurich:', stops_zurich.count())\n",
    "stops_zurich.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'8503000', {'lat': 47.3781762039461, 'lon': 8.54019357578468, 'name': 'Z\\xc3\\xbcrich HB'})"
     ]
    }
   ],
   "source": [
    "for r in stops_zurich.collect():\n",
    "    if r['stop_id'] == '8503000':\n",
    "        print((r['stop_id'], {'name': r['stop_name'].encode('utf-8'), 'lat': r['stop_lat'], 'lon': r['stop_lon']}))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = nx.MultiDiGraph()\n",
    "nodes = stops_zurich.rdd.map(lambda r: (r[0], {'name': r['stop_name'],\n",
    "                                              'lat': r['stop_lat'],\n",
    "                                              'lon': r['stop_lon']})).collect()\n",
    "graph.add_nodes_from(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o7826.showString.\n",
      ": java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1587988164357_1448/container_e05_1587988164357_1448_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 350, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1587988164357_1448/container_e05_1587988164357_1448_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1587988164357_1448/container_e05_1587988164357_1448_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1587988164357_1448/container_e05_1587988164357_1448_01_000001/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o7826.showString.\n",
      ": java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf\n",
    "def convertToMinute(s):\n",
    "    h, m, _ = s.split(':')\n",
    "    h,m = int(h), int(m)\n",
    "    \n",
    "    return h*60+m\n",
    "\n",
    "# Keep only travels around zurich\n",
    "nodes_list = list(graph.nodes())\n",
    "stop_times_zurich = stop_times.filter(column('stop_id').isin(nodes_list))\n",
    "stop_times_zurich = stop_times_zurich.filter(column('stop_id').isin(nodes_list))\n",
    "print('Number of stop times around zurich:', stop_times_zurich.count())\n",
    "# Convert time information to minutes elapsed since 0am\n",
    "stop_times_zurich = stop_times_zurich.withColumn('arrival_time', convertToMinute(column('arrival_time')))\n",
    "stop_times_zurich = stop_times_zurich.withColumn('departure_time', convertToMinute(column('departure_time')))\n",
    "# Add next stop to dataframe\n",
    "stop_times_zurich_2 = (stop_times_zurich.withColumn('stop_sequence_prev', column('stop_sequence')-1)\n",
    "                       .select('trip_id',\n",
    "                               column('stop_id').alias('next_stop'),\n",
    "                               column('stop_sequence_prev').alias('stop_sequence'),\n",
    "                               column('arrival_time').alias('next_arrival_time')))\n",
    "# Add trip duration\n",
    "stop_times_zurich = stop_times_zurich.join(stop_times_zurich_2, on=['trip_id', 'stop_sequence']).orderBy('trip_id', 'stop_sequence')\n",
    "stop_times_zurich = stop_times_zurich.withColumn('trip_duration', column('next_arrival_time')-column('departure_time'))\n",
    "stop_times_zurich = stop_times_zurich.select('stop_id', 'arrival_time', 'departure_time', 'next_stop', 'trip_duration')\n",
    "stop_times_zurich.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trip_duration = 2 #duration in hour \n",
    "def filter_edge_on_time(edges_df, day_id, hour, minute):\n",
    "    edges_df = edges_df.join(day_trips(day_id), on='trip_id')\n",
    "    arrival_minute = hour*60+minute\n",
    "    min_dep_time = arrival_minute - 60*60*max_trip_duration\n",
    "    edges_df = edges_df.filter((col('departure_minute') > min_dep_time) & \n",
    "                                            (col('arrival_minute') <= arrival_minute))\n",
    "\n",
    "    return edges_df\n",
    "\n",
    "# Example of graph construction: Wednesday arrival at 11:30:00\n",
    "edges = (filter_edge_on_time(edges_df, 2, 11, 30)\n",
    "         .rdd.map(lambda r: (r['stop_id'], r['next_stop'], {'duration': r['trip_duration'],\n",
    "                                                          'time': r['departure_minute']})).collect())\n",
    "print('Number of edges:', len(edges))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
