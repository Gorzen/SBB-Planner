{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'dslab-group_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7565</td><td>application_1589299642358_2060</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2060/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2060_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7585</td><td>application_1589299642358_2080</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2080/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2080_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7586</td><td>application_1589299642358_2081</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2081/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2081_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7587</td><td>application_1589299642358_2082</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2082/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2082_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7598</td><td>application_1589299642358_2093</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2093/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2093_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7599</td><td>application_1589299642358_2094</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2094/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2094_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7602</td><td>application_1589299642358_2097</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2097/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2097_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7603</td><td>application_1589299642358_2098</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2098/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2098_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7604</td><td>application_1589299642358_2099</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2099/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2099_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7606</td><td>application_1589299642358_2101</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2101/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2101_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7608</td><td>application_1589299642358_2103</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2103/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2103_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7609</td><td>application_1589299642358_2104</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2104/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2104_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7611</td><td>application_1589299642358_2106</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2106/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2106_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7615</td><td>application_1589299642358_2110</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/cluster/app/application_1589299642358_2110\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster054.iccluster.epfl.ch:8188/applicationhistory/logs/iccluster065.iccluster.epfl.ch:45454/container_e06_1589299642358_2110_01_000001/container_e06_1589299642358_2110_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7616</td><td>application_1589299642358_2111</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2111/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2111_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7617</td><td>application_1589299642358_2112</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2112/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2112_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7618</td><td>application_1589299642358_2113</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2113/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2113_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7619</td><td>application_1589299642358_2114</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2114/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2114_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"dslab-group_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7620</td><td>application_1589299642358_2115</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2115/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2115_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from heapq import heappush, heappop\n",
    "from itertools import count\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "MAX_TRIP_DURATION = 2 #duration in hour \n",
    "\n",
    "days_dict = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday'}\n",
    "def day_trips(*day_ids):\n",
    "    \"\"\"\n",
    "    day_trips: gives the trip_ids that operate on certain days\n",
    "    input: a variable number of day ids\n",
    "    output:s spark dataframe with trip_ids\n",
    "    \n",
    "    \"\"\"\n",
    "    days = [days_dict[day_id] for day_id in day_ids]\n",
    "    where_clause = \" and \".join(days)\n",
    "\n",
    "    day_services = calendar.where(where_clause).select('service_id')\n",
    "    return day_services.join(trips, on='service_id').select('trip_id')\n",
    "\n",
    "def minute_to_string(m):\n",
    "    hour, minute = m // 60, m - 60*(m//60)\n",
    "    time_string = '{:02}:{:02}'.format(int(hour), int(minute))\n",
    "    \n",
    "    return time_string\n",
    "\n",
    "def string_to_minute(s):\n",
    "    h, m, _ = s.split(':')\n",
    "    h,m = int(h), int(m)\n",
    "    \n",
    "    return h*60+m\n",
    "\n",
    "def normal_dijkstra(G, first_source, paths=None, cutoff=None, last_target=None):\n",
    "    \n",
    "    G_succ = G.succ if G.is_directed() else G.adj\n",
    "    paths = {first_source: [first_source]}\n",
    "\n",
    "    push = heappush\n",
    "    pop = heappop\n",
    "    dist = {}  # dictionary of final distances\n",
    "    \n",
    "    # dictionnary of wthether it's the first time a node is visited\n",
    "    seen = {first_source: 0}\n",
    "\n",
    "    c = count()\n",
    "    fringe = []  # use heapq with (distance,label) tuples\n",
    "    push(fringe, (0, next(c), first_source))\n",
    "    \n",
    "    while fringe:\n",
    "        #take the node to look at: \n",
    "        (d, _, source) = pop(fringe)\n",
    "        \n",
    "        # check if node has already been looked at: \n",
    "        if source in dist:\n",
    "            continue  # already searched this node.\n",
    "        \n",
    "        # update the distance of the node\n",
    "        dist[source] = d\n",
    "        \n",
    "        #stop if the node we look at is the target obviously\n",
    "        if source == last_target:\n",
    "            break\n",
    "            \n",
    "        # Look at all direct descendents from the source node: \n",
    "        for target, edges in G_succ[source].items():\n",
    "            # Because it's a multigraph, need to look at all edges between two nodes:\n",
    "            for edge_id in edges:\n",
    "                \n",
    "                # Get the duration between two nodes:\n",
    "                cost = graph.get_edge_data(source, target, edge_id)['duration']\n",
    "                \n",
    "                if cost is None:\n",
    "                        continue\n",
    "                \n",
    "                # Add the weight to the current distance of a node\n",
    "                current_dist = dist[source] + cost\n",
    "                \n",
    "                # if target has already been visited once and has a final distance:\n",
    "                if target in dist:\n",
    "                        # if we find a distance smaller than the actual distance in dic\n",
    "                        # raise error because dic distances contains only final distances\n",
    "                        if current_dist < dist[target]:\n",
    "                            raise ValueError('Contradictory paths found:',\n",
    "                                             'negative weights?')\n",
    "                # either node node been seen before or the current distance is smaller than the \n",
    "                # proposed distance in seen[target]:\n",
    "                elif target not in seen or current_dist < seen[target]:\n",
    "                    # update the seen distance\n",
    "                    seen[target] = current_dist\n",
    "                    # push it onto the heap so that we will look at its descendants later\n",
    "                    push(fringe, (current_dist, next(c), target))\n",
    "                    \n",
    "                    # update the paths till target:\n",
    "                    if paths is not None:\n",
    "                        paths[target] = paths[source] + [target]\n",
    "    if paths is not None:\n",
    "        return (dist, paths)\n",
    "    return dist\n",
    "\n",
    "def validate_path(path, confidence, graph):\n",
    "    #for _ in range(100):\n",
    "        #Validate path\n",
    "        #for e in path:\n",
    "            #sample_gaussian\n",
    "            #check if miss connection\n",
    "        #If > 0 connection missed, path missed\n",
    "    # if 95% must have missed < 5 path\n",
    "    # if path not validated -> starts with smaller threshold \n",
    "    return True\n",
    "\n",
    "def compute_delay_uncertainty(mean, std, confidence):\n",
    "    if confidence != None:\n",
    "        if mean == None or std == None:\n",
    "            return 0\n",
    "        \n",
    "        num_sample = 50 # how many (source, target, train_id, hour) tuple there is\n",
    "        t_quantile = stats.t(df=num_sample-1).ppf(confidence)\n",
    "        mean_deviation = t_quantile * std / np.sqrt(num_sample)\n",
    "        delay = mean_deviation\n",
    "    else:\n",
    "        delay = 0\n",
    "        \n",
    "    return delay\n",
    "\n",
    "def dijkstra_with_time(G, first_source, arrival_time, last_target=None, confidence=None, \n",
    "                       confidence_step=0.01, durations_dicts=None, paths=None):\n",
    "    G = G.copy()\n",
    "    departure_time = arrival_time - MAX_TRIP_DURATION*60\n",
    "    while True:\n",
    "        # Update durations according to confidence\n",
    "        if confidence != None:\n",
    "            if durations_dicts == None:\n",
    "                raise ValueError('You must pass durations_dicts for the confidence.')\n",
    "            # Load dict with modifications\n",
    "            if confidence not in durations_dicts:\n",
    "                edge_and_data_tuple = zip(G.edges(keys=True), \n",
    "                              map(lambda x: x[2], G.edges(data=True)))\n",
    "                edge_and_data_tuple = filter(lambda x: 'mean' in x[1] and 'std' in x[1], edge_and_data_tuple)\n",
    "                durations_dicts[confidence] = {e: {'duration': data['mean'] + compute_delay_uncertainty(data['mean'], \n",
    "                                                                                                        data['std'], \n",
    "                                                                                                        confidence)\n",
    "                                                   if data['mean'] != None and data['std'] != None\n",
    "                                                   else data['duration']\n",
    "                                                  } for e, data in edge_and_data_tuple}\n",
    "            \n",
    "            # Update graph\n",
    "            nx.set_edge_attributes(G, durations_dicts[confidence])\n",
    "        \n",
    "        \n",
    "        G_succ = G.succ if G.is_directed() else G.adj\n",
    "\n",
    "        paths = {first_source: [first_source]}\n",
    "        e_paths = {first_source: []}\n",
    "\n",
    "        push = heappush\n",
    "        pop = heappop\n",
    "        dist = {}  # dictionary of final distances\n",
    "\n",
    "        # dictionnary of whether it's the first time a node is visited\n",
    "        seen = {first_source: departure_time}\n",
    "\n",
    "\n",
    "        c = count()\n",
    "        fringe = []  # use heapq with (distance,label) tuples\n",
    "\n",
    "        #push(fringe, (0, next(c), first_source))\n",
    "        push(fringe, (departure_time, next(c), first_source))\n",
    "\n",
    "        while fringe:\n",
    "            #take the node to look at: \n",
    "            (d, _, source) = pop(fringe)\n",
    "            #print('Looking at node: '+source)\n",
    "\n",
    "            # check if node has already been looked at: \n",
    "            if source in dist:\n",
    "                continue  # already searched this node\n",
    "\n",
    "            # update the distance of the node\n",
    "            dist[source] = d\n",
    "\n",
    "            #stop if the node we look at is the target obviously\n",
    "            if source == last_target:\n",
    "                break\n",
    "\n",
    "                \n",
    "            # Look at all direct descendents from the source node: \n",
    "            for target, edges in G_succ[source].items():\n",
    "                # Because it's a multigraph, need to look at all edges between two nodes:\n",
    "                for edge_id in edges:\n",
    "                    # Check if walking edge\n",
    "                    dep_time_edge = G.get_edge_data(source, target, edge_id)['time']\n",
    "                    if dep_time_edge == -1:\n",
    "                        walking_edge = True\n",
    "                        current_trip_id = None\n",
    "                        dep_time_edge = d\n",
    "                    else:\n",
    "                        walking_edge = False\n",
    "                        current_trip_id = G.get_edge_data(source, target, edge_id)['trip_id']\n",
    "                        \n",
    "                    \n",
    "                    if dep_time_edge >= dist[source]:\n",
    "                        # Check if edge is feasible (also accoring to confidence)\n",
    "                        if len(e_paths[source]) >= 1 and not e_paths[source][-1][2]['walk']:\n",
    "                            last_edge_source, last_edge_target, last_edge_info = e_paths[source][-1]\n",
    "                            last_delay = compute_delay_uncertainty(last_edge_info['mean'], \n",
    "                                                                   last_edge_info['std'], \n",
    "                                                                   confidence)\n",
    "                            # If we make a transport-walk change, add delay to walk time\n",
    "                            if walking_edge:\n",
    "                                dep_time_edge += last_delay\n",
    "                            else:\n",
    "                                # If we make a transport-transport change, check if we have time\n",
    "                                if current_trip_id != last_edge_info['trip_id']\\\n",
    "                                and dep_time_edge < dist[source] + 2 + last_delay:\n",
    "                                    continue\n",
    "\n",
    "                        # Get the duration between two nodes:\n",
    "                        duration_cost = G.get_edge_data(source, target, edge_id)['duration']\n",
    "                        if duration_cost is None:\n",
    "                                continue\n",
    "\n",
    "                        # Add the weight to the current distance of a node\n",
    "                        current_dist = dep_time_edge + duration_cost\n",
    "\n",
    "                        # if target has already been visited once and has a final distance:\n",
    "                        if target in dist:\n",
    "                                # if we find a distance smaller than the actual distance in dic\n",
    "                                # raise error because dic distances contains only final distances\n",
    "                                if current_dist < dist[target]:\n",
    "                                    raise ValueError('Contradictory paths found:',\n",
    "                                                     'negative weights?')\n",
    "\n",
    "                        # either node has been seen before or the current distance is smaller than the \n",
    "                        # proposed distance in seen[target]:\n",
    "                        elif target not in seen or current_dist < seen[target]:\n",
    "                            # update the seen distance\n",
    "                            seen[target] = current_dist\n",
    "                            # push it onto the heap so that we will look at its descendants later\n",
    "                            push(fringe, (current_dist, next(c), target))\n",
    "\n",
    "                            # update the paths till target:\n",
    "                            if paths is not None:\n",
    "                                edge_dict = G.get_edge_data(source, target, edge_id)\n",
    "                                \n",
    "                                edge_dict['walk'] = walking_edge\n",
    "                                edge_dict['departure_time'] = dep_time_edge\n",
    "                                \n",
    "                                e_paths[target] = e_paths[source] + [(source, target, edge_dict)]\n",
    "\n",
    "\n",
    "        # No path exists\n",
    "        if  last_target not in e_paths:\n",
    "            print('Error: No paths to the source')\n",
    "            return pd.DataFrame(columns=['from', 'from_id', 'to', 'to_id', 'duration', 'total_duration',\n",
    "                                         'departure_time', 'walk', 'no_change', 'mean_std_null'])\n",
    "\n",
    "        \n",
    "        # Validation\n",
    "        if confidence == None or validate_path(e_paths[last_target], confidence, G):\n",
    "            break\n",
    "        else:\n",
    "            confidence += confidence_step\n",
    "            \n",
    "    # Path validated\n",
    "    if paths is not None:\n",
    "        nodes_data = G.nodes(data=True)\n",
    "        arrival_string = minute_to_string(dist[last_target])\n",
    "        best_path = e_paths[last_target]\n",
    "        departure_string = minute_to_string(best_path[0][2]['departure_time'])\n",
    "        print('Going from {} ({}) to {} ({}) in {:.2f} minutes, departure at {}'.format(nodes_data[first_source]['name'],\n",
    "                                                                                      first_source,\n",
    "                                                                                      nodes_data[last_target]['name'],\n",
    "                                                                                      last_target, \n",
    "                                                                                      dist[last_target] - departure_time,\n",
    "                                                                                      minute_to_string(departure_time)))\n",
    "        \n",
    "        # Construct best path's data structure\n",
    "        best_path_df = pd.DataFrame(columns=['from', 'from_id', 'to', 'to_id', 'duration', 'total_duration',\n",
    "                                          'departure_time', 'walk', 'no_change', 'mean_std_null'])\n",
    "        last_edge_info = False\n",
    "        for source, target, edge_info in best_path:\n",
    "            no_change = ('trip_id' in edge_info                                   # We're in a transport\n",
    "                         and last_edge_info and 'trip_id' in last_edge_info       # and last edge also\n",
    "                         and last_edge_info['trip_id'] == edge_info['trip_id'])   # and same trip_id\n",
    "            mean_std_null = 'trip_id' in edge_info and 'mean' not in edge_info or 'std' not in edge_info\n",
    "            \n",
    "            current_path_dict = {'from': nodes_data[source]['name'],\n",
    "                                 'from_id': source, \n",
    "                                 'to': nodes_data[target]['name'], \n",
    "                                 'to_id': target, \n",
    "                                 'duration': edge_info['duration'], \n",
    "                                 'total_duration': dist[target] - departure_time,\n",
    "                                 'departure_time': minute_to_string(edge_info['departure_time']), \n",
    "                                 'walk':edge_info['walk'], \n",
    "                                 'no_change': no_change, \n",
    "                                 'mean_std_null': mean_std_null}\n",
    "            best_path_df = best_path_df.append(current_path_dict, ignore_index=True)\n",
    "            last_edge_info = edge_info\n",
    "        \n",
    "        with pd.option_context('display.max_rows', None, \n",
    "                               'display.max_columns', None, \n",
    "                               'display.max_colwidth', 15,\n",
    "                               'display.expand_frame_repr', False):\n",
    "            print(best_path_df)\n",
    "        return best_path_df\n",
    "    raise ValueError('Should not be here')\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import pandas as pd\n",
    "username = os.environ['JUPYTERHUB_USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'username' as 'username' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trips = spark.read.format('orc').load('/data/sbb/timetables/orc/trips/000000_0')\n",
    "calendar = spark.read.format('orc').load('/data/sbb/timetables/orc/calendar/000000_0')\n",
    "\n",
    "nodes_df = spark.read.orc(\"/user/{}/nodes.orc\".format(username))\n",
    "edges_df = spark.read.orc(\"/user/{}/edges_with_mean_and_std_sec.orc\".format(username))\n",
    "\n",
    "#TODO: load durations_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = nodes_df.rdd.map(lambda r: (r[0], {'name': r['stop_name'],\n",
    "                                              'lat': r['stop_lat'],\n",
    "                                              'lon': r['stop_lon']})).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "walking_times = pd.read_pickle('walking_edges.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'walking_times' as 'walking_times' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%send_to_spark -i walking_times -t df -m 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reverse edges\n",
    "#edges_walking = (walking_times.withColumnRenamed('source', 'temp')\n",
    "#                 .withColumnRenamed('target', 'source')\n",
    "#                 .withColumnRenamed('temp', 'target').toPandas())\n",
    "edges_walking = walking_times.toPandas()\n",
    "edges_walking['attrs'] = edges_walking.apply(lambda x: {'time': -1, 'duration': x['walk_duration']}, axis=1)\n",
    "edges_walking = list(edges_walking[['source', 'target', 'attrs']].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save duration dictionnaries if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'durations_dicts' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'durations_dicts' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conversion to json\n",
    "durations_dicts_for_json = {}\n",
    "for c in durations_dicts.keys():\n",
    "    durations_dicts_for_json[c] = {str(k): v for k, v in durations_dicts[c].items()}\n",
    "\n",
    "print('Length of json:', len(json.dumps(durations_dicts_for_json)))\n",
    "\n",
    "# Save to hdfs\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose time of arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "day_id, arrival_hour, arrival_minute = 4, 12, 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_edges_for_trip(edges_df, day_id, arrival_time):\n",
    "    \"\"\"\n",
    "    create_edges_for_trip: constructs edges (and thus trips) that exist in a window of two hours before a given input time\n",
    "    @input:\n",
    "    - edges_df: df from which we construct the edges\n",
    "    - day_id: id of week-day (e.g. wednesday is day id 2, see dictionnary above)\n",
    "    - hour, minute: time at which we want to arrive somewhere (e.g. 11:30)\n",
    "    @output: data frame of selected edges\n",
    "    \"\"\"\n",
    "    #select only the trips that occur on that day:\n",
    "    edges_df= edges_df.join(day_trips(day_id), on='trip_id')\n",
    "    \n",
    "    min_dep_time = arrival_time - 60*MAX_TRIP_DURATION\n",
    "    \n",
    "    #keep only those in a window of two hours:\n",
    "    edges_df = edges_df.filter((col('departure_time') > min_dep_time) & \n",
    "                                            (col('arrival_time') <= arrival_time))\n",
    "    \n",
    "    #reverse edges\n",
    "    #edges_df = (edges_df.withColumnRenamed('next_stop', 'temp')\n",
    "    #            .withColumnRenamed('stop_id', 'next_stop')\n",
    "    #            .withColumnRenamed('temp', 'stop_id'))\n",
    "\n",
    "    edges = edges_df.rdd.map(lambda r: (r['stop_id'], r['next_stop'], {'duration': r['trip_duration'],\n",
    "                                                                       'time': float(r['departure_time']),\n",
    "                                                                       'trip_id': r['trip_id'],\n",
    "                                                                       'mean': r['mean'],\n",
    "                                                                       'std': r['std']})).collect()\n",
    "    \n",
    "    return edges + edges_walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edges = create_edges_for_trip(edges_df, day_id, arrival_hour*60+arrival_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 nodes removed"
     ]
    }
   ],
   "source": [
    "graph = nx.MultiDiGraph()\n",
    "graph.add_nodes_from(nodes)\n",
    "graph.add_edges_from(edges)\n",
    "\n",
    "old_number_of_nodes = graph.number_of_nodes()\n",
    "# Remove unreachable nodes\n",
    "dists, paths = normal_dijkstra(graph, '8503000')\n",
    "not_reachable = set(graph.nodes) - set(dists.keys())\n",
    "_ = graph.remove_nodes_from(list(not_reachable))\n",
    "print('{} nodes removed'.format(old_number_of_nodes - graph.number_of_nodes()))\n",
    "\n",
    "# Temp for problem of name's encoding\n",
    "import unicodedata\n",
    "nodes_data = graph.nodes(data=True)\n",
    "for n in graph.nodes:\n",
    "    nodes_data[n]['name'] = unicodedata.normalize('NFKD', nodes_data[n]['name']).encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp: creation of the dict with 0.95 and 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "durations_dicts = {}\n",
    "edge_and_data_tuple = zip(graph.edges(keys=True), \n",
    "              map(lambda x: x[2], graph.edges(data=True)))\n",
    "edge_and_data_tuple = filter(lambda x: 'mean' in x[1] and 'std' in x[1], edge_and_data_tuple)\n",
    "durations_dicts[0.98] = {e: {'duration': data['mean'] + compute_delay_uncertainty(data['mean'], \n",
    "                                                                                        data['std'], \n",
    "                                                                                        0.98)\n",
    "                                   if data['mean'] != None and data['std'] != None\n",
    "                                   else data['duration']\n",
    "                                  } for e, data in edge_and_data_tuple}\n",
    "durations_dicts[0.95] = {e: {'duration': data['mean'] + compute_delay_uncertainty(data['mean'], \n",
    "                                                                                        data['std'], \n",
    "                                                                                        0.95)\n",
    "                                   if data['mean'] != None and data['std'] != None\n",
    "                                   else data['duration']\n",
    "                                  } for e, data in edge_and_data_tuple}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without minimum confidence ->\n",
      "Going from Zurich HB (8503000) to Zurich, Auzelg (8591049) in 29.00 minutes, departure at 10:30\n",
      "             from         from_id              to           to_id  duration  total_duration departure_time   walk no_change mean_std_null\n",
      "0       Zurich HB         8503000       Zurich HB  8503000:0:4...  2.135259        2.135259          10:30   True     False          True\n",
      "1       Zurich HB  8503000:0:4...  Zurich Hard...     8503020:0:3  2.000000        9.000000          10:37  False     False         False\n",
      "2  Zurich Hard...     8503020:0:3  Zurich Oerl...     8503006:0:8  5.000000       14.000000          10:39  False      True         False\n",
      "3  Zurich Oerl...     8503006:0:8      Glattbrugg     8503310:0:3  2.000000       17.000000          10:45  False      True         False\n",
      "4      Glattbrugg     8503310:0:3  Glattbrugg,...         8590620  3.063448       20.063448          10:47   True     False          True\n",
      "5  Glattbrugg,...         8590620  Glattbrugg,...         8590626  1.000000       24.000000          10:53  False     False         False\n",
      "6  Glattbrugg,...         8590626  Glattpark, ...         8591830  2.000000       26.000000          10:54  False      True         False\n",
      "7  Glattpark, ...         8591830  Zurich, Fer...         8591128  1.000000       27.000000          10:56  False      True         False\n",
      "8  Zurich, Fer...         8591128  Zurich, Auzelg         8591049  2.000000       29.000000          10:57  False      True         False\n",
      "\n",
      "With minimum confidence ->\n",
      "Going from Zurich HB (8503000) to Zurich, Auzelg (8591049) in 30.95 minutes, departure at 10:30\n",
      "             from       from_id              to         to_id  duration  total_duration departure_time   walk no_change mean_std_null\n",
      "0       Zurich HB       8503000       Zurich HB  8503000:0:34  3.893616        3.893616          10:30   True     False          True\n",
      "1       Zurich HB  8503000:0:34  Zurich Oerl...   8503006:0:2  0.781612       12.781612          10:42  False     False         False\n",
      "2  Zurich Oerl...   8503006:0:2     Wallisellen   8503129:0:3  3.000000       21.000000          10:48  False      True         False\n",
      "3     Wallisellen   8503129:0:3  Wallisellen...       8587655  2.943438       23.943438          10:51   True     False          True\n",
      "4  Wallisellen...       8587655  Wallisellen...       8590881  0.951107       27.951107          10:57  False     False         False\n",
      "5  Wallisellen...       8590881  Wallisellen...       8590889  0.608539       28.608539          10:58  False      True         False\n",
      "6  Wallisellen...       8590889  Zurich, Auzelg       8591049  0.947184       30.947184          11:00  False      True         False"
     ]
    }
   ],
   "source": [
    "# Tao's example (except for the departure time)\n",
    "print('Without minimum confidence ->')\n",
    "best_path1 = dijkstra_with_time(graph, '8503000', arrival_hour*60+arrival_minute, last_target='8591049')\n",
    "print('\\nWith minimum confidence ->')\n",
    "best_path2 = dijkstra_with_time(graph, '8503000', arrival_hour*60+arrival_minute, last_target='8591049', confidence=0.98, durations_dicts=durations_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without minimum confidence ->\n",
      "Going from Zurich, Triemli (8503610) to Zurich Altstetten, Bahnhof N (8591057) in 17.14 minutes, departure at 10:30\n",
      "             from  from_id              to    to_id  duration  total_duration departure_time   walk no_change mean_std_null\n",
      "0  Zurich, Tri...  8503610  Zurich, In ...  8591214  1.000000        2.000000          10:31  False     False         False\n",
      "1  Zurich, In ...  8591214  Zurich, Gol...  8591163  1.000000        3.000000          10:32  False      True         False\n",
      "2  Zurich, Gol...  8591163  Zurich, Alb...  8591036  2.000000        5.000000          10:33  False      True         False\n",
      "3  Zurich, Alb...  8591036  Zurich, Alb...  8591037  0.000000        5.000000          10:35  False      True         False\n",
      "4  Zurich, Alb...  8591037  Zurich, Unt...  8591408  2.000000        7.000000          10:35  False      True         False\n",
      "5  Zurich, Unt...  8591408  Zurich, Rau...  8591311  1.000000        8.000000          10:37  False      True         False\n",
      "6  Zurich, Rau...  8591311  Zurich, Lin...  8591258  2.000000       10.000000          10:38  False      True         False\n",
      "7  Zurich, Lin...  8591258  Zurich, Bri...  8591097  1.000000       11.000000          10:40  False      True         False\n",
      "8  Zurich, Bri...  8591097  Zurich Alts...  8591056  1.000000       12.000000          10:41  False      True         False\n",
      "9  Zurich Alts...  8591056  Zurich Alts...  8591057  5.143597       17.143597          10:42   True     False          True\n",
      "\n",
      "With minimum confidence ->\n",
      "Going from Zurich, Triemli (8503610) to Zurich Altstetten, Bahnhof N (8591057) in 38.87 minutes, departure at 10:30\n",
      "              from         from_id              to           to_id  duration  total_duration departure_time   walk no_change mean_std_null\n",
      "0   Zurich, Tri...         8503610  Zurich, Sch...         8580912  1.538114        3.538114          10:32  False     False         False\n",
      "1   Zurich, Sch...         8580912  Zurich, Gol...         8502572  1.702670        7.702670          10:36  False     False         False\n",
      "2   Zurich, Gol...         8502572  Zurich, Sch...         8591341  1.724206        9.724206          10:38  False      True         False\n",
      "3   Zurich, Sch...         8591341  Zurich Wied...         8573710  1.857909       11.857909          10:40  False      True         False\n",
      "4   Zurich Wied...         8573710  Zurich Wied...     8503011:0:2  3.122926       15.485801          10:42   True     False          True\n",
      "5   Zurich Wied...     8503011:0:2       Zurich HB    8503000:0:34  1.445433       20.445433          10:49  False     False         False\n",
      "6        Zurich HB    8503000:0:34       Zurich HB         8503000  3.893616       24.561425          10:50   True     False          True\n",
      "7        Zurich HB         8503000       Zurich HB  8503000:0:4...  2.135259       26.696684          10:54   True     False          True\n",
      "8        Zurich HB  8503000:0:4...  Zurich Hard...     8503020:0:4  0.705851       29.705851          10:59  False     False         False\n",
      "9   Zurich Hard...     8503020:0:4  Zurich Alts...     8503001:0:3  4.000000       35.000000          11:01  False      True         False\n",
      "10  Zurich Alts...     8503001:0:3  Zurich Alts...         8591057  3.870416       38.870416          11:05   True     False          True"
     ]
    }
   ],
   "source": [
    "# From Triemli to Altstetten\n",
    "print('Without minimum confidence ->')\n",
    "best_path1 = dijkstra_with_time(graph, '8503610', arrival_hour*60+arrival_minute, last_target='8591057')\n",
    "print('\\nWith minimum confidence ->')\n",
    "best_path2 = dijkstra_with_time(graph, '8503610', arrival_hour*60+arrival_minute, last_target='8591057', confidence=0.95, durations_dicts=durations_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration': 2.0, 'std': 1.05697167217, 'time': 682.0, 'trip_id': u'234.TA.26-15-j19-1.41.H', 'mean': 0.27319172912666667}"
     ]
    }
   ],
   "source": [
    "# Weird attributes?\n",
    "print(graph.get_edge_data('8503000:0:41/42', '8503020:0:3', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046632828786368166"
     ]
    }
   ],
   "source": [
    "# Proportion of null mean or std in non-walking edges\n",
    "(len(filter(lambda x: x[2]['mean'] == None or x[2]['std'] == None, filter(lambda x: 'mean' in x[2] and 'std' in x[2], graph.edges(data=True))))\n",
    " / float(len(filter(lambda x: 'mean' in x[2] and 'std' in x[2], graph.edges(data=True)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
